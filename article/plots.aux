\relax 
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training a three layer network with 8,8,4 linear activation units on the hadamard gate. Depicted are 50 training runs with random initialization and different sets of training samples.}}{1}}
\newlabel{fig:ltl}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Training different rotation gates sequentially on pure states. The same network consisting of 8,8,4 real linear units was trained to fit different rotation gates sequentially on multiples of $1/8 \pi .$ After the first training sequence has converged subsequent training runs begin from a lower starting loss.}}{2}}
\newlabel{fig:ltl}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training a network to predict density matrices produced by a hadamard gate. Two training curves are shown in the upper figure and the corresponding mean and standard deviation of the traces of the predicted density matrices in the lower figure. The traces of the predicted matrices are evaluated every 30th training step. The network architecture consists of 16,3,16 linear real units, thereby forcing the network to make use of the correct number of degrees of freedom.}}{3}}
\newlabel{fig:density_hadamard}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Training a network to predict density matrices produced by a CNOT gate. Two training curves are shown in the upper figure and the corresponding mean and standard deviation of the traces of the predicted density matrices in the lower figure. The traces of the predicted matrices are evaluated every 30th training step. The network architecture consistsof 64,16,64 linear real units.}}{4}}
\newlabel{fig:density_hadamard}{{4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Training a one layer relu network on shifted data to prevent dying relus.}}{5}}
\newlabel{fig:relu_hadamard_4}{{5}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Training a two layer relu network of 8,4 units on shifted data to prevent dying relus.}}{5}}
\newlabel{fig:relu_hadamard_8_4}{{6}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Training a three layer network with 4Bit quantized weights and linear activation units. The network is fully connected with 8,8,4 units at each layer. Shown are 50 training histories.}}{6}}
\newlabel{fig:relu_hadamard_8_4}{{7}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Training to predict hadamard gate transformation of 10000 randomly selected pure states. Depicted in the upper figure are linear regression and worse performing training on non-linear activation units. Batch size of 32 and adagrad optimizer. The lower figure depicts training on the same training data set with a 3 three layer network and different activation units (linear, elu, leaky relu). Again using a batch size of 32 and adagrad optimizer. }}{7}}
\newlabel{fig:relu_hadamard_8_4}{{8}{7}}
